{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "available-lighting",
   "metadata": {},
   "source": [
    "# Regression Tree Implementation from Scratch: Proof-of Concept\n",
    "Decision Trees are very flexible models that are very prone to overfitting the training data without constraints in place. Typically, the algorithm can be constrained on by how \"deep\" the tree is allowed to be, what the minimum required of number of samples in a node is, in order for it to be split and by cost-complexity pruning, which essentially adds a penalty to the objective function, which increases the more flexible the tree is (i.e. the more nodes it has). For the purpose of this project, I stuck with using the second option: A node can only be split if it has at least X samples in it. If it has fewer than this, it becomes a leaf and can not be split further. The objective function to minimize is the mean squared error. Knowledge of Decision Trees is largely assumed and will not be explained here. For a list of sources I used to learn what is implemented here, see the references at the end.\n",
    "\n",
    "This implementation of the CART algorithm is probably not a very good one from a computational efficiency point of view, but this is just a proof of concept and I am not computer scientist, software engineer or data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "scientific-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "supported-plate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 34: expected 9 fields, saw 10\\nSkipping line 128: expected 9 fields, saw 10\\nSkipping line 332: expected 9 fields, saw 10\\nSkipping line 338: expected 9 fields, saw 10\\nSkipping line 356: expected 9 fields, saw 10\\nSkipping line 376: expected 9 fields, saw 10\\n'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('auto-mpg.csv', error_bad_lines=False, index_col='car name') # skip the lines with missing values that cause errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "proprietary-relevance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chevrolet chevelle malibu</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buick skylark 320</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plymouth satellite</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amc rebel sst</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ford torino</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           %mpg  cylinders  displacement  horsepower  weight  \\\n",
       "car name                                                                       \n",
       "chevrolet chevelle malibu  18.0          8         307.0       130.0  3504.0   \n",
       "buick skylark 320          15.0          8         350.0       165.0  3693.0   \n",
       "plymouth satellite         18.0          8         318.0       150.0  3436.0   \n",
       "amc rebel sst              16.0          8         304.0       150.0  3433.0   \n",
       "ford torino                17.0          8         302.0       140.0  3449.0   \n",
       "\n",
       "                           acceleration  model year  origin  \n",
       "car name                                                     \n",
       "chevrolet chevelle malibu          12.0          70       1  \n",
       "buick skylark 320                  11.5          70       1  \n",
       "plymouth satellite                 11.0          70       1  \n",
       "amc rebel sst                      12.0          70       1  \n",
       "ford torino                        10.5          70       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-riding",
   "metadata": {},
   "source": [
    "## The RegresionTree Class \n",
    "The algorithm sorts on every column sequentially, sets a threshold between every two datapoints for a given column (so first between observation 1 and 2, then between 2 and 3, and so on and so forth). For every threshold/split, an average value for the independent variable is computed, along with its MSE. The threshold that reduces the MSE is chosen, the data is split into two partitions and the same procedure is repeated for both of those partitions, which in turn can produce up to 4 new partitons (provided the two partitions created by the first split have enough samples in them to be split again). The processes continues until all nodes too few samples in them to be split further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "mature-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree:\n",
    "\n",
    "    def __init__(self, min_sample_split):\n",
    "        self.min_sample_split = min_sample_split\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        try:\n",
    "            self.no_cols = self.X.shape[1]\n",
    "\n",
    "        except:\n",
    "            self.X = self.X.reshape((len(self.X), 1))\n",
    "            self.no_cols = 1\n",
    "\n",
    "        # Have to merge X y so that we can keep track of y when we make conditional selections on X\n",
    "        self.data = np.c_[self.X, self.y]\n",
    "        self.tree = self.grow_tree(data=self.data)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        node = self.tree\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] <= node.cutoff:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        return node.predicted_value\n",
    "\n",
    "    def get_best_split(self, data):\n",
    "\n",
    "        self.best_cutoffs = np.array([])\n",
    "        self.lowest_mses = np.array([])\n",
    "\n",
    "        for i in range(self.no_cols):\n",
    "\n",
    "            self.cutoff_array = np.array([])\n",
    "            self.mse_array = np.array([])\n",
    "\n",
    "            for j in range(len(data[:, i]) - 1):\n",
    "\n",
    "                self.sorted_data = data[data[:, i].argsort()]\n",
    "                self.cutoff = np.mean(self.sorted_data[:, i][j:j + 2])\n",
    "\n",
    "                if (len(self.sorted_data[:, -1]) >= self.min_sample_split):\n",
    "\n",
    "                    self.average_y_left = np.mean(self.sorted_data[self.sorted_data[:, i] <= self.cutoff][:, -1])\n",
    "\n",
    "                    # To avoid RuntimeWarnings\n",
    "                    if np.isnan(self.sorted_data[self.sorted_data[:, i] > self.cutoff][:, -1]).size == 0:\n",
    "                        break\n",
    "\n",
    "                    self.average_y_right = np.mean(self.sorted_data[self.sorted_data[:, i] > self.cutoff][:, -1])\n",
    "                    self.mse_left = np.sum(\n",
    "                        ((self.sorted_data[self.sorted_data[:, i] <= self.cutoff][:, -1] - self.average_y_left) ** 2))\n",
    "                    self.mse_right = np.sum(\n",
    "                        ((self.sorted_data[self.sorted_data[:, i] > self.cutoff][:, -1] - self.average_y_right) ** 2))\n",
    "                    self.mse = (self.mse_left + self.mse_right)/len(self.sorted_data)\n",
    "                    self.mse_array = np.append(self.mse_array, self.mse)\n",
    "                    self.cutoff_array = np.append(self.cutoff_array, self.cutoff)\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            try:\n",
    "                self.best_cutoffs = np.append(self.best_cutoffs, self.cutoff_array[self.mse_array.argmin()])\n",
    "                self.lowest_mses = np.append(self.lowest_mses, np.min(self.mse_array))\n",
    "\n",
    "            except:  # Return None None if we didn't split the data due to min_sample_split constraint not being met\n",
    "                return None, None\n",
    "\n",
    "        self.the_best_cutoff = self.best_cutoffs[np.argmin(self.lowest_mses)]\n",
    "        self.best_var_to_split_on = self.lowest_mses.argmin()\n",
    "\n",
    "        return self.the_best_cutoff, self.best_var_to_split_on\n",
    "\n",
    "    def grow_tree(self, data):\n",
    "        \n",
    "        class Node:  # class to keep track of the growing tree\n",
    "            def __init__(self, predicted_value):\n",
    "                self.predicted_value = predicted_value\n",
    "                self.feature_index = None\n",
    "                self.cutoff = None\n",
    "                self.left = None\n",
    "                self.right = None\n",
    "                self.left_data = None\n",
    "                self.right_data = None\n",
    "\n",
    "        self.predicted_value = np.mean(data[:, -1])\n",
    "        node = Node(predicted_value=self.predicted_value)\n",
    "        self.threshold, self.idx = self.get_best_split(data)\n",
    "        if self.idx is not None:\n",
    "            self.right_idxs = data[:, self.idx] > self.threshold\n",
    "            self.left_idxs = data[:, self.idx] <= self.threshold\n",
    "            node.left_data = data[self.left_idxs]\n",
    "            node.right_data = data[self.right_idxs]\n",
    "            node.feature_index = self.idx\n",
    "            node.cutoff = self.threshold\n",
    "            node.left = self.grow_tree(data=node.left_data)\n",
    "            node.right = self.grow_tree(data=node.right_data)\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "indonesian-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['horsepower', 'weight']].values\n",
    "y = df['%mpg'].values\n",
    "data = np.c_[X,y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-coordinate",
   "metadata": {},
   "source": [
    "Note: I was pretty tired of this project by the time I got this far, so it is not very robust or stable. It cannot, for instance, make predictions on an array of samples. I.e. one can only pass individual samples into the TreeRegression class. So, in order to make multiple predictions at once, we will make a very unpythonic loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "owned-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.67971014492754\n",
      "25.459420289855075\n",
      "14.721428571428572\n",
      "22.32\n"
     ]
    }
   ],
   "source": [
    "y_test = [\n",
    "    [1,1],\n",
    "    [90,2000],\n",
    "    [128, 3000],\n",
    "    [80, 2800],\n",
    "]\n",
    "\n",
    "test = np.array(test)\n",
    "\n",
    "model = RegressionTree(min_sample_split=100)\n",
    "model.fit(X,y)\n",
    "\n",
    "for i in test:\n",
    "    res = model.predict(i)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-platform",
   "metadata": {},
   "source": [
    "As a sanity check, let's check these results against scikit-learn's DecisionTreeRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "compatible-answer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.67971014492754\n",
      "25.45942028985507\n",
      "14.721428571428572\n",
      "22.319999999999997\n"
     ]
    }
   ],
   "source": [
    "true_model = DecisionTreeRegressor(min_samples_split=100)\n",
    "true_model.fit(X,y)\n",
    "result = true_model.predict(y_test)\n",
    "\n",
    "for i in result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-bailey",
   "metadata": {},
   "source": [
    "# References \n",
    "Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Springer Science & Business Media.\n",
    "\n",
    "James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: springer.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
